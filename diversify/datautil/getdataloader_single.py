# -*- coding: utf-8 -*-
"""getdataloader_single

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uSiNLnBw08cd76bud0D4CIWofCacWbOa
"""

# Copyright (c) Microsoft Corporation.
# Licensed under the MIT License.

# coding=utf-8
import numpy as np
from torch.utils.data import DataLoader

import datautil.actdata.util as actutil
from datautil.util import combindataset, subdataset

import datautil.actdata.cross_people as cross_people
import torch
from gnn.graph_builder import build_correlation_graph

task_act = {'cross_people': cross_people}

def collate_fn(batch):
    """Custom collate function that also computes edge_index for each sample if using GNN."""
    inputs, labels, domains, cls_labels, pd_labels, index = zip(*batch)

    inputs = torch.stack(inputs)  # shape [B, C, 1, T]
    labels = torch.tensor(labels)
    domains = torch.tensor(domains)
    cls_labels = torch.tensor(cls_labels)
    pd_labels = torch.tensor(pd_labels)
    index = torch.tensor(index)

    # GNN: Transform inputs â†’ [B, T, C]
    inputs_reshaped = inputs.squeeze(2).permute(0, 2, 1)  # (B, T, C)

    # Compute edge_index for each sample using correlation
    edge_indices = [build_correlation_graph(sample.cpu().numpy()) for sample in inputs_reshaped]

    return inputs, labels, domains, cls_labels, pd_labels, index, edge_indices



def get_dataloader(args, tr, val, tar):
    train_loader = DataLoader(dataset=tr, batch_size=args.batch_size,
                          num_workers=args.N_WORKERS, drop_last=False, shuffle=True,
                          collate_fn=collate_fn)

    train_loader_noshuffle = DataLoader(dataset=tr, batch_size=args.batch_size,
                                    num_workers=args.N_WORKERS, drop_last=False, shuffle=False,
                                    collate_fn=collate_fn)

    valid_loader = DataLoader(dataset=val, batch_size=args.batch_size,
                          num_workers=args.N_WORKERS, drop_last=False, shuffle=False,
                          collate_fn=collate_fn)

    target_loader = DataLoader(dataset=tar, batch_size=args.batch_size,
                           num_workers=args.N_WORKERS, drop_last=False, shuffle=False,
                           collate_fn=collate_fn)

                           
    return train_loader, train_loader_noshuffle, valid_loader, target_loader


def get_act_dataloader(args):
    source_datasetlist = []
    target_datalist = []
    pcross_act = task_act[args.task]

    tmpp = args.act_people[args.dataset]
    args.domain_num = len(tmpp)
    for i, item in enumerate(tmpp):
        tdata = pcross_act.ActList(
            args, args.dataset, args.data_dir, item, i, transform=actutil.act_train())
        if i in args.test_envs:
            target_datalist.append(tdata)
        else:
            source_datasetlist.append(tdata)
            if len(tdata)/args.batch_size < args.steps_per_epoch:
                args.steps_per_epoch = len(tdata)/args.batch_size
    rate = 0.2
    args.steps_per_epoch = int(args.steps_per_epoch*(1-rate))
    tdata = combindataset(args, source_datasetlist)
    l = len(tdata.labels)
    indexall = np.arange(l)
    np.random.seed(args.seed)
    np.random.shuffle(indexall)
    ted = int(l*rate)
    indextr, indexval = indexall[ted:], indexall[:ted]
    tr = subdataset(args, tdata, indextr)
    val = subdataset(args, tdata, indexval)
    targetdata = combindataset(args, target_datalist)
    train_loader, train_loader_noshuffle, valid_loader, target_loader = get_dataloader(
        args, tr, val, targetdata)
    return train_loader, train_loader_noshuffle, valid_loader, target_loader, tr, val, targetdata
